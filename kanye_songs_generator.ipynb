{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import to_categorical\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# from unidecode import unidecode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random, sys, io, re, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_reader = csv.reader(open('data/Lana_del_rey/lyrics.csv', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a dictionary of song: lyrics\n",
    "def get_tokenized_lines(csv):\n",
    "    lyrics = {}\n",
    "    for r in csv:\n",
    "        words = []\n",
    "        row = str(r[2]).lower()\n",
    "        for line in row.split('|-|'):\n",
    "            new_words = re.findall(r\"\\b[a-z']+\\b\", line)\n",
    "            words = words + new_words\n",
    "        lyrics[r[1]] = words\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_lyric_words = get_tokenized_lines(csv_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total word number: 173631, total lines of lyrics: 22688, average word per line: 7.652988\n",
    "\n",
    "SEQ_LENGTH = 32 + 1 # this one will be deleted during prediction, \"1\" stands for using a sequence to generate one word\n",
    "sequences = list()\n",
    "\n",
    "def get_all_sequences():\n",
    "    for song in all_lyric_words:\n",
    "        if len(all_lyric_words[song]) < SEQ_LENGTH:\n",
    "            sequences.append(all_lyric_words[song])\n",
    "        else:\n",
    "            for i in range(SEQ_LENGTH, len(all_lyric_words[song])):\n",
    "                seq = all_lyric_words[song][i - SEQ_LENGTH: i]\n",
    "                sequences.append(seq)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 66117\n"
     ]
    }
   ],
   "source": [
    "sequences = get_all_sequences()\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 3810\n"
     ]
    }
   ],
   "source": [
    "# store all the unique words and match them with indices\n",
    "all_words = []\n",
    "for song in all_lyric_words:\n",
    "    for word in all_lyric_words[song]:\n",
    "        all_words.append(word)\n",
    "unique_word = set(all_words)\n",
    "word_to_index = {w: i for i, w in enumerate(unique_word)}\n",
    "index_to_word = {i: w for w, i in word_to_index.items()}\n",
    "word_indices = [word_to_index[word] for word in unique_word]\n",
    "word_size = len(unique_word)\n",
    "\n",
    "print('vocabulary size: {}'.format(word_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funtion change the words into matrix wise data, and each position of the matrix stands for the index of the word in index_to_word\n",
    "\n",
    "def data_to_matrix(lines, seq_len):\n",
    "    matrix = np.zeros((len(lines), seq_len))\n",
    "    for r, line in enumerate(lines):\n",
    "        for c, word in enumerate(line):\n",
    "            matrix[r, c] = word_to_index[word]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_data = data_to_matrix(sequences, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66117,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_data[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the y = Wx data, X is the input data, and y is the target data\n",
    "X, y = matrix_data[:, :-1], matrix_data[:, -1]\n",
    "y = to_categorical(y, num_classes=word_size) # to_categorical: for categorical_crossentropy optimiser\n",
    "seq_length = len(X[0]) # 32 in our case, stands for sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (66117, 32)\n",
      "y_shape (66117, 3810)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_shape\", X.shape)\n",
    "print(\"y_shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 32, 32)            121920    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32, 100)           53200     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3810)              384810    \n",
      "=================================================================\n",
      "Total params: 650,430\n",
      "Trainable params: 650,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "66117/66117 [==============================] - 129s - loss: 6.1180 - acc: 0.0419   \n",
      "Epoch 2/50\n",
      "66117/66117 [==============================] - 128s - loss: 5.6319 - acc: 0.0562   \n",
      "Epoch 3/50\n",
      "66117/66117 [==============================] - 128s - loss: 5.3521 - acc: 0.0744   \n",
      "Epoch 4/50\n",
      "66117/66117 [==============================] - 128s - loss: 5.1439 - acc: 0.0887   \n",
      "Epoch 5/50\n",
      "66117/66117 [==============================] - 128s - loss: 4.9209 - acc: 0.1076   \n",
      "Epoch 6/50\n",
      "66117/66117 [==============================] - 129s - loss: 4.6767 - acc: 0.1315   \n",
      "Epoch 7/50\n",
      "66117/66117 [==============================] - 129s - loss: 4.4478 - acc: 0.1550   \n",
      "Epoch 8/50\n",
      "66117/66117 [==============================] - 129s - loss: 4.2351 - acc: 0.1793   \n",
      "Epoch 9/50\n",
      "66117/66117 [==============================] - 129s - loss: 4.0291 - acc: 0.2069   \n",
      "Epoch 10/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.8446 - acc: 0.2296   \n",
      "Epoch 11/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.6704 - acc: 0.2560   \n",
      "Epoch 12/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.5165 - acc: 0.2777   \n",
      "Epoch 13/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.5428 - acc: 0.2808   \n",
      "Epoch 14/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.6774 - acc: 0.2610   \n",
      "Epoch 15/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.6171 - acc: 0.2583   \n",
      "Epoch 16/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.4402 - acc: 0.2848   \n",
      "Epoch 17/50\n",
      "66117/66117 [==============================] - 131s - loss: 3.3218 - acc: 0.3030   \n",
      "Epoch 18/50\n",
      "66117/66117 [==============================] - 130s - loss: 3.1844 - acc: 0.3244   \n",
      "Epoch 19/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.0719 - acc: 0.3460   \n",
      "Epoch 20/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.0400 - acc: 0.3512   \n",
      "Epoch 21/50\n",
      "66117/66117 [==============================] - 129s - loss: 3.1341 - acc: 0.3326   \n",
      "Epoch 22/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.9722 - acc: 0.3654   \n",
      "Epoch 23/50\n",
      "66117/66117 [==============================] - 130s - loss: 2.8575 - acc: 0.3849   \n",
      "Epoch 24/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.7469 - acc: 0.4069   \n",
      "Epoch 25/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.6506 - acc: 0.4254   \n",
      "Epoch 26/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.5585 - acc: 0.4421   \n",
      "Epoch 27/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.4694 - acc: 0.4603   \n",
      "Epoch 28/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.3873 - acc: 0.4758   \n",
      "Epoch 29/50\n",
      "66117/66117 [==============================] - 130s - loss: 2.3103 - acc: 0.4898   \n",
      "Epoch 30/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.2351 - acc: 0.5040   \n",
      "Epoch 31/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.1634 - acc: 0.5184   \n",
      "Epoch 32/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.1012 - acc: 0.5290   \n",
      "Epoch 33/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.0307 - acc: 0.5418   \n",
      "Epoch 34/50\n",
      "66117/66117 [==============================] - 130s - loss: 1.9755 - acc: 0.5550   \n",
      "Epoch 35/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.9132 - acc: 0.5671   \n",
      "Epoch 36/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.8579 - acc: 0.5763   \n",
      "Epoch 37/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.8009 - acc: 0.5910   \n",
      "Epoch 38/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.7498 - acc: 0.6002   \n",
      "Epoch 39/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.6987 - acc: 0.6090   \n",
      "Epoch 40/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.9122 - acc: 0.5810   \n",
      "Epoch 41/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.4752 - acc: 0.4604   \n",
      "Epoch 42/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.3617 - acc: 0.4720   \n",
      "Epoch 43/50\n",
      "66117/66117 [==============================] - 130s - loss: 2.1891 - acc: 0.5049   \n",
      "Epoch 44/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.0642 - acc: 0.5305   \n",
      "Epoch 45/50\n",
      "66117/66117 [==============================] - 129s - loss: 2.0154 - acc: 0.5413   \n",
      "Epoch 46/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.9388 - acc: 0.5575   \n",
      "Epoch 47/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.8679 - acc: 0.5701   \n",
      "Epoch 48/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.8110 - acc: 0.5835   \n",
      "Epoch 49/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.7138 - acc: 0.6045   \n",
      "Epoch 50/50\n",
      "66117/66117 [==============================] - 129s - loss: 1.8131 - acc: 0.5844   \n"
     ]
    }
   ],
   "source": [
    "# establish the network, using LSTM and compile it\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 32, input_length=seq_length)) # Embedding(input_dim, output_dim, input_length), which respecively stands for: the possible value of the word, the output vector size, sequence length\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(word_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "adam = Adam(0.002)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history = model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5') # this now stores the model for kanye west's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfer text to a numpy matrix\n",
    "def text_to_matrix(texts, word_to_index):\n",
    "    indices = np.zeros((1, len(texts)), dtype=int)\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        indices[:, i] = word_to_index[text]\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure seach sequence is no longer than max length\n",
    "def my_pad_sequences(seq, max_length):\n",
    "    start = seq.shape[1] - max_length\n",
    "    return seq[:, start: start + max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate lyrics\n",
    "def generate_text(model, word_to_index, seq_length, seed_text, num_of_words):\n",
    "    result = list()[1:]\n",
    "    input_text = seed_text.lower()\n",
    "\n",
    "    for num in range(num_of_words):\n",
    "        encoded = text_to_matrix(input_text.split(), word_to_index)\n",
    "        encoded = my_pad_sequences(encoded, max_length=seq_length)\n",
    "        predict = model.predict_classes(encoded, verbose=0)\n",
    "        out_word = ''\n",
    "    \n",
    "        for word, index in word_to_index.items():\n",
    "            if index == predict:\n",
    "                out_word = word\n",
    "                break\n",
    "        \n",
    "        input_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done my hair up real big beauty queen style high heels and don't wanna fall in love with a in love with you but i lost myself when i lost you but i still don't tell me hey of a dime of bay like you seem and now you miss\n"
     ]
    }
   ],
   "source": [
    "# this is for lana del rey\n",
    "# make sure the seed text has length 32 and without puntuations\n",
    "seed_text = \"kiss me hard before you go summertime sadness I just wanted you to know that baby you the best I got my red dress on tonight dancing in the dark in the pale moonlight\"\n",
    "generated = generate_text(model, word_to_index, SEQ_LENGTH - 1, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was gassin me or just a train blaow blang my niggas holding a bad bitch and you know what i did i was on a mil million seconds flash and breathe and breathe and breathe and breathe and breathe and calm down and i know you know i was three\n"
     ]
    }
   ],
   "source": [
    "# this is what we got for Kanye West !!!\n",
    "seed_text = \"Bougie girl grab her hand fuck that bitch she don't wanna dance excuse my French but I'm in France Prince williams ain't do it right if you ask me cause if I\"\n",
    "generated = generate_text(model, word_to_index, SEQ_LENGTH - 1, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "weights = model.layers[4].get_weights()[0].reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_3d(weights):\n",
    "    figure = plt.figure(figsize=(20,10))\n",
    "    ax = figure.add_subplot(111, projection='3d')\n",
    "    for i in range(0, len(weights)):\n",
    "        ax.scatter(weights[i, 0], weights[i, 1], weights[i, 2], s = 75)\n",
    "        ax.text(weights[i,0], weights[i,1],weights[i,2], '%s' % (str(i)), size = 20, zorder = 1, color = 'k')\n",
    "    plt.show()\n",
    "\n",
    "plot_3d(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
