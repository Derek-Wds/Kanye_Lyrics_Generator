{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import to_categorical\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "import random, sys, io, re, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = csv.reader(open('data\\\\lyrics.csv', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dictionary of song: lyrics\n",
    "def get_tokenized_lines(csv):\n",
    "    lyrics = {}\n",
    "    for r in csv:\n",
    "        words = []\n",
    "        row = str(r[2]).lower()\n",
    "        for line in row.split('|-|'):\n",
    "            new_words = re.findall(r\"\\b[a-z']+\\b\", unidecode(line))\n",
    "            words = words + new_words\n",
    "        lyrics[r[1]] = words\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyric_words = get_tokenized_lines(csv_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total word number: 173631, total lines of lyrics: 22688, average word per line: 7.652988\n",
    "SEQ_LENGTH = 23\n",
    "sequences = list()\n",
    "\n",
    "def get_all_sequences():\n",
    "    for song in all_lyric_words:\n",
    "        if len(all_lyric_words[song]) < SEQ_LENGTH:\n",
    "            sequences.append(all_lyric_words[song])\n",
    "        else:\n",
    "            for i in range(SEQ_LENGTH, len(all_lyric_words[song])):\n",
    "                seq = all_lyric_words[song][i - SEQ_LENGTH: i]\n",
    "                sequences.append(seq)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 166036\n"
     ]
    }
   ],
   "source": [
    "sequences = get_all_sequences()\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 9681\n"
     ]
    }
   ],
   "source": [
    "# store all the unique words and match them with indices\n",
    "all_words = []\n",
    "for song in all_lyric_words:\n",
    "    for word in all_lyric_words[song]:\n",
    "        all_words.append(word)\n",
    "unique_word = set(all_words)\n",
    "word_to_index = {w: i for i, w in enumerate(unique_word)}\n",
    "index_to_word = {i: w for w, i in word_to_index.items()}\n",
    "word_indices = [word_to_index[word] for word in unique_word]\n",
    "word_size = len(unique_word)\n",
    "\n",
    "print('vocabulary size: {}'.format(word_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this funtion change the words into matrix wise data, and each position of the matrix stands for the index of the word in index_to_word\n",
    "\n",
    "def data_to_matrix(lines, seq_len):\n",
    "    matrix = np.zeros((len(lines), seq_len))\n",
    "    \n",
    "    for r, line in enumerate(lines):\n",
    "        for c, word in enumerate(line):\n",
    "            matrix[r, c] = word_to_index[word]\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0a457e8889f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatrix_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-41b88a1de70b>\u001b[0m in \u001b[0;36mdata_to_matrix\u001b[0;34m(lines, seq_len)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "matrix_data = data_to_matrix(sequences, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166375,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_data[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the y = Wx data, X\n",
    "X, y = matrix_data[:, :], matrix_data[:, -1]\n",
    "y = to_categorical(y, num_classes=word_size)\n",
    "seq_length = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (166375, 21)\n",
      "y_shape (166375, 9681)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_shape\", X.shape)\n",
    "print(\"y_shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
